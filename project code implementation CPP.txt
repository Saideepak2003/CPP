# -*- coding: utf-8 -*-
"""1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VeDcAkGqx4A00OvOiTmjFoQy8ZSml_qU
"""

# Commented out IPython magic to ensure Python compatibility.
#Packages related to general operating system & warnings
import os 
import warnings
warnings.filterwarnings('ignore')
#Packages related to data importing, manipulation, exploratory data #analysis, data understanding
import numpy as np
import pandas as pd
from pandas import Series, DataFrame
from termcolor import colored as cl # text customization
#Packages related to data visualizaiton
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
#Setting plot sizes and type of plot
plt.rc("font", size=14)
plt.rcParams['axes.grid'] = True
plt.figure(figsize=(6,3))
plt.gray()
from matplotlib.backends.backend_pdf import PdfPages
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn import metrics
from sklearn.impute import MissingIndicator, SimpleImputer
from sklearn.preprocessing import  PolynomialFeatures, KBinsDiscretizer, FunctionTransformer
from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer, OrdinalEncoder
import statsmodels.formula.api as smf
import statsmodels.tsa as tsa
from sklearn.linear_model import LogisticRegression, LinearRegression, ElasticNet, Lasso, Ridge
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.ensemble import BaggingClassifier, BaggingRegressor,RandomForestClassifier,RandomForestRegressor
from sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor, AdaBoostClassifier, AdaBoostRegressor 
from sklearn.svm import LinearSVC, LinearSVR, SVC, SVR
from xgboost import XGBClassifier
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

data=pd.read_csv("/content/creditcard.csv")

Total_transactions = len(data)
normal = len(data[data.Class == 0])
fraudulent = len(data[data.Class == 1])
fraud_percentage = round(fraudulent/normal*100, 2)
print(cl('Total number of Trnsactions are {}'.format(Total_transactions), attrs = ['bold']))
print(cl('Number of Normal Transactions are {}'.format(normal), attrs = ['bold']))
print(cl('Number of fraudulent Transactions are {}'.format(fraudulent), attrs = ['bold']))
print(cl('Percentage of fraud Transactions is {}'.format(fraud_percentage), attrs = ['bold']))

data.info()

min(data.Amount),max(data.Amount)

sc = StandardScaler()
amount = data['Amount'].values
data['Amount'] = sc.fit_transform(amount.reshape(-1, 1))

data.drop(['Time'], axis=1, inplace=True)

data.shape

data.drop_duplicates(inplace=True)

data.shape

X = data.drop('Class', axis = 1).values
y = data['Class'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 1)

DT = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')
DT.fit(X_train, y_train)
dt_yhat = DT.predict(X_test)

print('Accuracy score of the Decision Tree model is {}'.format(accuracy_score(y_test, dt_yhat)))

print('F1 score of the Decision Tree model is {}'.format(f1_score(y_test, dt_yhat)))

confusion_matrix(y_test, dt_yhat, labels = [0, 1])



n = 7
KNN = KNeighborsClassifier(n_neighbors = n)
KNN.fit(X_train, y_train)
knn_yhat = KNN.predict(X_test)

print('Accuracy score of the K-Nearest Neighbors model is {}'.format(accuracy_score(y_test, knn_yhat)))

print('F1 score of the K-Nearest Neighbors model is {}'.format(f1_score(y_test, knn_yhat)))

lr = LogisticRegression()
lr.fit(X_train, y_train)
lr_yhat = lr.predict(X_test)

print('Accuracy score of the Logistic Regression model is {}'.format(accuracy_score(y_test, lr_yhat)))

print('F1 score of the Logistic Regression model is {}'.format(f1_score(y_test, lr_yhat)))

import pandas as pd

df = pd.read_csv('/content/creditcard.csv')

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

# Split the dataset into input and output
X = df.iloc[:, :-1]
y = df.iloc[:, -1]

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train an SVM model
svm = SVC()
svm.fit(X_train, y_train)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, f1_score

# Load the credit card transaction data into a Pandas DataFrame
df = pd.read_csv('/content/creditcard.csv')

# Split the dataset into input and output
X = df.iloc[:, :-1]
y = df.iloc[:, -1]

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train an SVM model
svm = SVC()
svm.fit(X_train, y_train)

# Evaluate the performance of the model using accuracy and F1 score
y_pred = svm.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Accuracy:", accuracy)
print("F1 score:", f1)

print('F1 score of the Support Vector Machines model is {}'.format(f1_score(y_test, y_pred)))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score

# Load the credit card transaction data into a Pandas DataFrame
df = pd.read_csv('/content/creditcard.csv')

# Split the dataset into input and output
X = df.iloc[:, :-1]
y = df.iloc[:, -1]

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a random forest model
rf = RandomForestClassifier(max_depth=4)
rf.fit(X_train, y_train)

# Evaluate the performance of the model using accuracy and F1 score
y_pred = rf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Accuracy score of the Random Forest model is", accuracy)
print("F1 score of the Random Forest model is", f1)

import pandas as pd
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, f1_score

# Load the credit card transaction data into a Pandas DataFrame
df = pd.read_csv('/content/creditcard.csv')

# Split the dataset into input and output
X = df.iloc[:, :-1]
y = df.iloc[:, -1]

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a gradient boosting model
xgb = XGBClassifier(max_depth=4)
xgb.fit(X_train, y_train)

# Evaluate the performance of the model using accuracy and F1 score
y_pred = xgb.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Accuracy score of the XGBoost model is", accuracy)
print("F1 score of the XGBoost model is", f1)

import pandas as pd
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Load the credit card transaction data into a Pandas DataFrame
df = pd.read_csv('/content/creditcard.csv')

# Split the dataset into input and output
X = df.iloc[:, :-1]
y = df.iloc[:, -1]

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a gradient boosting model
xgb = XGBClassifier(max_depth=4)
xgb.fit(X_train, y_train)

# Generate predictions on the test set
y_pred = xgb.predict(X_test)

# Generate a confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Generate a classification report
cr = classification_report(y_test, y_pred)

# Print the confusion matrix and classification report
print("Confusion matrix:\n", cm)
print("\nClassification report:\n", cr)

# Visualize the confusion matrix using a heatmap
sns.heatmap(cm, annot=True, cmap="Blues", fmt="d")
plt.title("Confusion matrix for XGBoost model")
plt.xlabel("Predicted class")
plt.ylabel("True class")
plt.show()

# Visualize the classification report using a bar chart
cr_df = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).T
f1_scores = cr_df["f1-score"]
f1_scores.plot(kind="bar", color="blue")
plt.title("F1 scores for XGBoost model")
plt.xlabel("Class")
plt.ylabel("F1 score")
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Load the housing price data into a Pandas DataFrame
df = pd.read_csv('/content/creditcard.csv')

# Split the dataset into input and output
X = df.iloc[:, :-1]
y = df.iloc[:, -1]

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
lr = LinearRegression()
lr.fit(X_train, y_train)

# Generate predictions on the test set
y_pred = lr.predict(X_test)

# Calculate the mean squared error of the predictions
mse = mean_squared_error(y_test, y_pred)

# Print the mean squared error
print("Mean squared error:", mse)

# Visualize the predicted values versus the actual values using a scatter plot
plt.scatter(y_test, y_pred)
plt.title("Predicted vs. actual values for linear regression model")
plt.xlabel("Actual values")
plt.ylabel("Predicted values")
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# Load the creditcard.csv data into a Pandas DataFrame
df = pd.read_csv('/content/creditcard.csv')

# Split the dataset into input and output
X = df.iloc[:, :-1]
y = df.iloc[:, -1]

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
lr = LinearRegression()
lr.fit(X_train, y_train)

# Generate predictions on the test set
y_pred = lr.predict(X_test)

# Create a scatter plot of the actual values versus the predicted values
plt.scatter(y_test, y_pred)
plt.xlabel('Actual values')
plt.ylabel('Predicted values')
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# Load the creditcard.csv data into a Pandas DataFrame
df = pd.read_csv('/content/creditcard.csv')

# Split the dataset into input and output
X = df.iloc[:, :-1]
y = df.iloc[:, -1]

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a KNN classifier
knn = KNeighborsClassifier()
knn.fit(X_train, y_train)

# Generate predictions on the test set
y_pred = knn.predict(X_test)

# Create a scatter plot of the training data, color-coded by class
sns.scatterplot(x=X_train.iloc[:,0], y=X_train.iloc[:,1], hue=y_train, palette='deep')
plt.xlabel(X_train.columns[0])
plt.ylabel(X_train.columns[1])
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt

# Load the creditcard.csv data into a Pandas DataFrame
df = pd.read_csv('/content/creditcard.csv')

# Split the dataset into input and output
X = df.iloc[:, :-1]
y = df.iloc[:, -1]

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a decision tree classifier
dt = DecisionTreeClassifier(max_depth=3)
dt.fit(X_train, y_train)

# Plot the decision tree
fig, ax = plt.subplots(figsize=(12, 8))
plot_tree(dt, ax=ax, filled=True, feature_names=X.columns, class_names=['0', '1'])
plt.show()

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression


# Train the logistic regression model
lr = LogisticRegression()
lr.fit(X_train, y_train)

# Make predictions on the test set
y_pred = lr.predict(X_test)

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Visualize the confusion matrix
fig, ax = plt.subplots(figsize=(8, 8))
ax.imshow(cm)
ax.grid(False)
ax.set_xlabel('Predicted outputs', fontsize=12, color='black')
ax.set_ylabel('True outputs', fontsize=12, color='black')
ax.xaxis.set(ticks=(0, 1), ticklabels=('Non-Fraud', 'Fraud'))
ax.yaxis.set(ticks=(0, 1), ticklabels=('Non-Fraud', 'Fraud'))
ax.set_ylim(1.5, -0.5)
for i in range(2):
    for j in range(2):
        ax.text(j, i, cm[i, j], ha='center', va='center', color='white', fontsize=18)
plt.show()

